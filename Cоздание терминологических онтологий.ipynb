{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import bigrams, trigrams\n",
    "import math\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "with codecs.open('blockchainhealthcarereview.txt', \"r\",encoding='utf-8', errors='ignore') as fdata:\n",
    "    data = fdata.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "285"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq(word, doc):\n",
    "    return doc.count(word)\n",
    "\n",
    "def word_count(doc):\n",
    "    return len(doc)\n",
    "\n",
    "def tf(word, doc):\n",
    "    return (freq(word, doc) / float(word_count(doc)))\n",
    "\n",
    "def num_docs_containing(word, list_of_docs):\n",
    "    count = 0\n",
    "    for document in list_of_docs:\n",
    "        if freq(word, document) > 0:\n",
    "            count += 1\n",
    "    return 1 + count\n",
    "\n",
    "def idf(word, list_of_docs):\n",
    "    return math.log(len(list_of_docs) /\n",
    "            float(num_docs_containing(word, list_of_docs)))\n",
    "\n",
    "def tf_idf(word, doc, list_of_docs):\n",
    "    return (tf(word, doc) * idf(word, list_of_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "285it [00:33,  8.55it/s]\n"
     ]
    }
   ],
   "source": [
    "#Compute the frequency for each term.\n",
    "vocabulary = []\n",
    "docs = {}\n",
    "all_tips = []\n",
    "\n",
    "full_text_single = []\n",
    "full_text_bi = []\n",
    "full_text_tri = []\n",
    "\n",
    "for i,tip in tqdm.tqdm(enumerate(data)):\n",
    "    tokens = [token for token in tokenizer.tokenize(tip) if not token.isdigit()]\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    tokens = [wordnet_lemmatizer.lemmatize(token) for token in tokens if token not in stopwords]\n",
    "    \n",
    "    \n",
    "    bi_tokens = bigrams(tokens)\n",
    "    tri_tokens = trigrams(tokens)    \n",
    "\n",
    "    bi_tokens = [' '.join(token) for token in bi_tokens]\n",
    "#     bi_tokens = [token for token in bi_tokens]\n",
    "\n",
    "    tri_tokens = [' '.join(token) for token in tri_tokens]\n",
    "#     tri_tokens = [token for token in tri_tokens]\n",
    "\n",
    "    final_tokens = []\n",
    "    final_tokens.extend(tokens)\n",
    "    final_tokens.extend(bi_tokens)\n",
    "    final_tokens.extend(tri_tokens)\n",
    "    \n",
    "    full_text_single.append(tokens)\n",
    "    full_text_bi.append(bi_tokens)\n",
    "    full_text_tri.append(tri_tokens)\n",
    "    \n",
    "    docs[i] = {'freq': {}, 'tf': {}, 'idf': {},\n",
    "                        'tf-idf': {}, 'tokens': []}\n",
    "\n",
    "    for token in final_tokens:\n",
    "        #The frequency computed for each tip\n",
    "        docs[i]['freq'][token] = freq(token, final_tokens)\n",
    "        #The term-frequency (Normalized Frequency)\n",
    "        docs[i]['tf'][token] = tf(token, final_tokens)\n",
    "        docs[i]['tokens'] = final_tokens\n",
    "\n",
    "    vocabulary.append(final_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 285/285 [1:45:38<00:00, 22.24s/it]\n"
     ]
    }
   ],
   "source": [
    "for doc in tqdm.tqdm(docs):\n",
    "    for token in docs[doc]['tf']:\n",
    "        docs[doc]['idf'][token] = idf(token, vocabulary)\n",
    "        docs[doc]['tf-idf'][token] = tf_idf(token, docs[doc]['tokens'], vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 285/285 [00:00<00:00, 641.71it/s]\n"
     ]
    }
   ],
   "source": [
    "#Now let's find out the most relevant words by tf-idf.\n",
    "words = {}\n",
    "for doc in tqdm.tqdm(docs):\n",
    "    for token in docs[doc]['tf-idf']:\n",
    "        if token not in words:\n",
    "            words[token] = docs[doc]['tf-idf'][token]\n",
    "        else:\n",
    "            if docs[doc]['tf-idf'][token] > words[token]:\n",
    "                words[token] = docs[doc]['tf-idf'][token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def horizon_visibility(tseries):\n",
    "    lit = []\n",
    "    for i in range(0,len(tseries)-1):\n",
    "        founded_r = False\n",
    "        founded_l = False\n",
    "        \n",
    "        (ta,ya)=tseries[i]\n",
    "            \n",
    "        for n in range(i+1,len(tseries)):\n",
    "            (tb,yb)=tseries[n]\n",
    "            if(yb>=ya):\n",
    "                founded_r = True\n",
    "                lit.append([ta,tb])\n",
    "                break\n",
    "        if not founded_r:\n",
    "#             print(min([tseries[n] for n in range(i+1,len(tseries))],key=lambda x:abs(ya-x[1])))\n",
    "            right_min = (min([tseries[n] for n in range(i+1,len(tseries))],key=lambda x:abs(ya-x[1])))\n",
    "            for n in range(i+1,len(tseries)):\n",
    "                (tb,yb)=tseries[n]\n",
    "                if yb==right_min:\n",
    "                    lit.append([ta,tb])\n",
    "                    break\n",
    "                    \n",
    "        for n in range(i-1,0,-1):\n",
    "            (tb,yb)=tseries[n]\n",
    "            if(yb>=ya):\n",
    "                founded_l = True\n",
    "                lit.append([ta,tb])\n",
    "                break\n",
    "        if not founded_l and i>1:\n",
    "#             print(min([tseries[n] for n in range(i-1,0,-1)],key=lambda x:abs(ya-x[1])))\n",
    "            left_min = (min([tseries[n] for n in range(i-1,0,-1)],key=lambda x:abs(ya-x[1])))\n",
    "            for n in range(i-1,0,-1):\n",
    "                (tb,yb)=tseries[n]\n",
    "                if(yb==left_min):\n",
    "                    lit.append([ta,tb])\n",
    "                    break\n",
    "        elif not founded_l and i==1:\n",
    "            lit.append([1,tseries[0][0]])\n",
    "\n",
    "    return lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TXT_single = sum(full_text_single,[])\n",
    "TXT_bi = sum(full_text_bi,[])\n",
    "TXT_tri = sum(full_text_tri,[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tup = [[i,words[w]] for i,w in enumerate(TXT_bi)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 37s, sys: 4.87 s, total: 5min 42s\n",
      "Wall time: 5min 44s\n"
     ]
    }
   ],
   "source": [
    "% time lit = horizon_visibility(tup)\n",
    "# dump = lit.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,_ in enumerate(lit):\n",
    "    lit[i] = [TXT_bi[lit[i][0]],TXT_bi[lit[i][1]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "h=nx.Graph()\n",
    "h.add_edges_from(lit)\n",
    "# nx.draw(h,with_labels=True)\n",
    "# pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_single_words = [x[0] for x in sorted(h.degree(), key=lambda x: x[1], reverse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_bigram_words = [x[0] for x in sorted(h.degree(), key=lambda x: x[1], reverse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_trigram_words = [x[0] for x in sorted(h.degree(), key=lambda x: x[1], reverse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22923"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(top_bigram_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import random\n",
    "import string\n",
    "import networkx as nx\n",
    "import matplotlib\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [03:58<00:00,  2.09it/s]\n"
     ]
    }
   ],
   "source": [
    "file = open('more_new.csv','w')\n",
    "for u in tqdm.tqdm(top_single_words[:N]):\n",
    "    for b in top_bigram_words[:N]:        \n",
    "        for t in top_trigram_words[:N]:\n",
    "            if u in b.split():\n",
    "                file.write(u+','+b+'\\n')\n",
    "            elif u in t.split():\n",
    "                file.write(u+','+t+'\\n')\n",
    "#                 print(u,t)\n",
    "            elif b in t:\n",
    "                file.write(b+','+t+'\\n')\n",
    "#                 print(u,b,t) \n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
